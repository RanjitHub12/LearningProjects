{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a74381",
   "metadata": {},
   "source": [
    "# PHASE 1: SETUP & DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb256c5",
   "metadata": {},
   "source": [
    "#### Cell 1: Install Dependencies\n",
    "#### Installs everything needed for Mamba, visualization, and efficiency tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c161d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 1: Installs ---\n",
    "# 1. System dependencies for visualization\n",
    "!apt-get install -y graphviz\n",
    "\n",
    "# 2. Python libraries\n",
    "# Uninstall potential conflicts first\n",
    "!pip uninstall -y mamba-ssm causal-conv1d\n",
    "\n",
    "print(\"--- Installing Deep Learning Extensions... ---\")\n",
    "!pip install causal-conv1d==1.5.0 --no-deps --no-build-isolation\n",
    "!pip install mamba-ssm==2.2.4 --no-deps --no-build-isolation\n",
    "\n",
    "print(\"--- Installing Analysis Tools... ---\")\n",
    "!pip install -q graphviz captum thop\n",
    "\n",
    "print(\"\\n‚úÖ Environment Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ede65e3",
   "metadata": {},
   "source": [
    "#### Cell 2: Imports & Configuration\n",
    "#### All libraries and global variables in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Imports & Configuration ---\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import graphviz\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "# PyTorch & ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, average_precision_score, \n",
    "    f1_score, confusion_matrix, classification_report, \n",
    "    roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "from thop import profile # For FLOPs counting\n",
    "\n",
    "# Mamba Import\n",
    "try:\n",
    "    from mamba_ssm.modules.mamba_simple import Mamba\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Warning: Mamba not imported. Ensure Cell 1 ran successfully.\")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = \"/kaggle/input/200k-real-vs-ai-visuals-by-mbilal/my_real_vs_ai_dataset/my_real_vs_ai_dataset\"\n",
    "TRAIN_CSV = \"/kaggle/input/200k-real-vs-ai-visuals-by-mbilal/train_labels.csv\"\n",
    "VAL_CSV   = \"/kaggle/input/200k-real-vs-ai-visuals-by-mbilal/val_labels.csv\"\n",
    "TEST_CSV  = \"/kaggle/input/200k-real-vs-ai-visuals-by-mbilal/test_labels.csv\"\n",
    "CKPT_DIR  = \"/kaggle/working/checkpoints\"\n",
    "\n",
    "# External Data (Optional)\n",
    "EXT_DATA_ROOT = \"/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake\"\n",
    "EXT_TEST_CSV  = \"/kaggle/input/140k-real-and-fake-faces/test.csv\"\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "print(f\"üöÄ Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9460d61",
   "metadata": {},
   "source": [
    "#### Cell 3: Data Loading & Transformations\n",
    "#### Defines the Dataset class and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c06d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: Data Loading ---\n",
    "\n",
    "# 1. Transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.0),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.3),\n",
    "    transforms.RandomAdjustSharpness(1.5, p=0.3),\n",
    "    transforms.RandomApply([transforms.RandomAffine(degrees=0, translate=(0.05,0.05))], p=0.4),\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "# 2. Dataset Class\n",
    "class GravexDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_root, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_path = os.path.join(self.img_root, row['filename'])\n",
    "        \n",
    "        # Fallback for subfolders\n",
    "        if not os.path.exists(img_path):\n",
    "            for sub in [\"real\", \"ai_images\"]:\n",
    "                alt_path = os.path.join(self.img_root, sub, row['filename'])\n",
    "                if os.path.exists(alt_path):\n",
    "                    img_path = alt_path\n",
    "                    break\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "             # Return dummy for broken images\n",
    "             return torch.zeros((3, 224, 224)), torch.tensor(-1.0, dtype=torch.float32)\n",
    "\n",
    "        label = torch.tensor(int(row['label']), dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 3. Initialize Loaders\n",
    "train_dataset = GravexDataset(TRAIN_CSV, DATA_ROOT, transform=train_transform)\n",
    "val_dataset   = GravexDataset(VAL_CSV, DATA_ROOT, transform=val_transform)\n",
    "test_dataset  = GravexDataset(TEST_CSV, DATA_ROOT, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Data Ready: Train({len(train_dataset)}), Val({len(val_dataset)}), Test({len(test_dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562c54d",
   "metadata": {},
   "source": [
    "#### Cell 4: Model Architecture\n",
    "#### Defines the ResNet+Mamba model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Model Architecture ---\n",
    "\n",
    "class ResNetMambaDetector(nn.Module):\n",
    "    def __init__(self, embed_dim=128, d_state=64, device=None):\n",
    "        super().__init__()\n",
    "        self.backbone = ResNetBackbone(out_dim=embed_dim)\n",
    "        # Pass device to Mamba if provided (needed for CPU/GPU switching in efficiency tests)\n",
    "        self.mamba = Mamba(d_model=embed_dim, d_state=d_state, expand=1, d_conv=4, device=device)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 64), nn.ReLU(), nn.Dropout(0.3), nn.Linear(64, 1)\n",
    "        )\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.backbone(x)           # (B, 128)\n",
    "        seq = emb.unsqueeze(1)           # (B, 1, 128)\n",
    "        out = self.mamba(seq).squeeze(1) # (B, 128)\n",
    "        return self.classifier(out).squeeze(-1)\n",
    "\n",
    "# Instantiate Model\n",
    "model = ResNetMambaDetector(embed_dim=128, d_state=64, device=DEVICE).to(DEVICE)\n",
    "print(\"‚úÖ Model Architecture Defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c8dce",
   "metadata": {},
   "source": [
    "#### Cell 5: Visualize Architecture\n",
    "#### Generates the Graphviz diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Visualize Architecture ---\n",
    "def generate_architecture_diagram():\n",
    "    dot = graphviz.Digraph(comment='ResNet-Mamba Detector')\n",
    "    dot.attr(rankdir='LR', label='ResNet-Mamba Architecture', fontsize='20')\n",
    "    \n",
    "    # Styles\n",
    "    styles = {\n",
    "        'input': {'shape': 'box', 'style': 'filled', 'fillcolor': '#a9def9'},\n",
    "        'backbone': {'shape': 'box', 'style': 'filled', 'fillcolor': '#e4c1f9'},\n",
    "        'mamba': {'shape': 'box', 'style': 'filled', 'fillcolor': '#f694c1'},\n",
    "        'head': {'shape': 'box', 'style': 'filled', 'fillcolor': '#c3f73a'},\n",
    "        'op': {'shape': 'oval', 'style': 'filled', 'fillcolor': 'lightgrey'}\n",
    "    }\n",
    "\n",
    "    dot.node('In', 'Input\\n(B, 3, 224, 224)', **styles['input'])\n",
    "    dot.node('RN', 'ResNet-18\\nBackbone', **styles['backbone'])\n",
    "    dot.node('Pr', 'Linear Proj\\n(512->128)', **styles['backbone'])\n",
    "    \n",
    "    with dot.subgraph(name='cluster_mamba') as c:\n",
    "        c.attr(label='Mamba Block', style='dashed')\n",
    "        c.node('Unsq', 'Unsqueeze', **styles['op'])\n",
    "        c.node('Mamba', 'Mamba Layer', **styles['mamba'])\n",
    "        c.node('Sq', 'Squeeze', **styles['op'])\n",
    "        c.edge('Unsq', 'Mamba')\n",
    "        c.edge('Mamba', 'Sq')\n",
    "\n",
    "    with dot.subgraph(name='cluster_head') as c:\n",
    "        c.attr(label='Classifier', style='dashed')\n",
    "        c.node('FC', 'Linear+ReLU', **styles['head'])\n",
    "        c.node('Out', 'Logit', **styles['head'])\n",
    "        c.edge('FC', 'Out')\n",
    "\n",
    "    dot.edge('In', 'RN')\n",
    "    dot.edge('RN', 'Pr')\n",
    "    dot.edge('Pr', 'Unsq')\n",
    "    dot.edge('Sq', 'FC')\n",
    "\n",
    "    try:\n",
    "        dot.render('model_arch', format='png', view=False)\n",
    "        display(IPImage(filename='model_arch.png'))\n",
    "    except Exception as e:\n",
    "        print(f\"Graphviz Error: {e}\")\n",
    "\n",
    "generate_architecture_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b046cdc",
   "metadata": {},
   "source": [
    "# PHASE 2: TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bc39e",
   "metadata": {},
   "source": [
    "#### Cell 6: Training Loop & Helpers\n",
    "#### Runs the training process and saves checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce27bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Training Loop ---\n",
    "\n",
    "# Setup\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.amp.GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "history = {'train_loss': [], 'train_auc': [], 'val_loss': [], 'val_auc': [], 'val_acc': []}\n",
    "best_val_auc = 0.0\n",
    "\n",
    "def save_checkpoint(epoch, model, val_auc, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'val_auc': val_auc\n",
    "    }, path)\n",
    "\n",
    "print(f\"üöÄ Starting Training for {EPOCHS} Epochs...\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "    for xb, yb in pbar:\n",
    "        xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda', enabled=(DEVICE.type == \"cuda\")):\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        preds.extend(torch.sigmoid(out).detach().cpu().numpy().tolist())\n",
    "        trues.extend(yb.detach().cpu().numpy().tolist())\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_auc = roc_auc_score(trues, preds)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    v_preds, v_trues = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\", leave=False):\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            with torch.amp.autocast(device_type='cuda', enabled=(DEVICE.type == \"cuda\")):\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            v_preds.extend(torch.sigmoid(out).cpu().numpy().tolist())\n",
    "            v_trues.extend(yb.cpu().numpy().tolist())\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_auc = roc_auc_score(v_trues, v_preds)\n",
    "    val_acc = accuracy_score(v_trues, (np.array(v_preds) > 0.5).astype(int))\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, AUC={train_auc:.4f} | Val Loss={val_loss:.4f}, AUC={val_auc:.4f}\")\n",
    "\n",
    "    # Log History\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_auc'].append(train_auc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    # Save Best\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        save_checkpoint(epoch, model, val_auc, os.path.join(CKPT_DIR, f\"best_model_auc{val_auc:.4f}.pth\"))\n",
    "\n",
    "print(\"‚úÖ Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2382e35",
   "metadata": {},
   "source": [
    "#### Cell 7: Plot Training History\n",
    "#### Interpolates history to handle potential gaps and plots curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Training History ---\n",
    "if len(history['train_loss']) > 0:\n",
    "    df_hist = pd.DataFrame(history)\n",
    "    df_hist = df_hist.interpolate(method='linear', limit_direction='both') # Fill gaps\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(df_hist.index + 1, df_hist['train_loss'], 'b-', label='Train Loss')\n",
    "    plt.plot(df_hist.index + 1, df_hist['val_loss'], 'r--', label='Val Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':')\n",
    "\n",
    "    # Metrics\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(df_hist.index + 1, df_hist['train_auc'], 'b-', label='Train AUC')\n",
    "    plt.plot(df_hist.index + 1, df_hist['val_auc'], 'r--', label='Val AUC')\n",
    "    plt.plot(df_hist.index + 1, df_hist['val_acc'], 'g:', label='Val Acc')\n",
    "    plt.title('Metric Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':')\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No history to plot yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741c0d1",
   "metadata": {},
   "source": [
    "# PHASE 3: MASTER INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd5b09",
   "metadata": {},
   "source": [
    "#### Cell 8: Master Inference (Run Once, Use Everywhere)\n",
    "#### Loads the best model and runs inference on the Test Set. Stores results in memory for all subsequent plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98051c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 8: MASTER INFERENCE ---\n",
    "# We run this ONCE so we don't have to re-predict for every plot.\n",
    "\n",
    "# 1. Load Best Model\n",
    "model_files = glob.glob(os.path.join(CKPT_DIR, \"best_epoch*_auc*.pth\"))\n",
    "if model_files:\n",
    "    best_path = max(model_files, key=lambda x: float(x.split('auc')[1].split('.pth')[0].replace('_', '.')))\n",
    "    print(f\"üì• Loading Best Model: {best_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(best_path, map_location=DEVICE, weights_only=False)\n",
    "    state = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint\n",
    "    model.load_state_dict(state)\n",
    "    model.to(DEVICE).eval()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No checkpoint found. Using current model state.\")\n",
    "\n",
    "# 2. Run Inference on 200k Test Set\n",
    "print(\"üß™ Running Inference on 200k Test Set...\")\n",
    "all_preds_200k, all_labels_200k = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in tqdm(test_loader, desc=\"Inference\"):\n",
    "        valid = (lbls != -1)\n",
    "        if not valid.any(): continue\n",
    "        \n",
    "        imgs, lbls = imgs[valid].to(DEVICE), lbls[valid].to(DEVICE)\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda', enabled=(DEVICE.type == \"cuda\")):\n",
    "            out = model(imgs)\n",
    "            \n",
    "        probs = torch.sigmoid(out).squeeze().cpu().numpy().tolist()\n",
    "        # Handle single-item batch case where tolist() returns float\n",
    "        if isinstance(probs, float): probs = [probs]\n",
    "            \n",
    "        all_preds_200k.extend(probs)\n",
    "        all_labels_200k.extend(lbls.cpu().numpy().tolist())\n",
    "\n",
    "# Convert to numpy for easy plotting later\n",
    "all_labels_200k = np.array(all_labels_200k)\n",
    "all_preds_probs_200k = np.array(all_preds_200k)\n",
    "all_preds_classes_200k = (all_preds_probs_200k > 0.5).astype(int)\n",
    "\n",
    "print(f\"\\n‚úÖ Inference Done. Loaded {len(all_labels_200k)} predictions into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1cf358",
   "metadata": {},
   "source": [
    "#### Cell 9: External Inference (Optional)\n",
    "#### Runs inference on the secondary 140k dataset for generalization checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 9: External Inference (140k Dataset) ---\n",
    "if os.path.exists(EXT_TEST_CSV):\n",
    "    print(\"üß™ Running Inference on External 140k Dataset...\")\n",
    "    \n",
    "    # Helper Dataset class for external data\n",
    "    class ExternalDataset(Dataset):\n",
    "        def __init__(self, csv, root, transform):\n",
    "            self.df = pd.read_csv(csv)\n",
    "            self.root = root\n",
    "            self.transform = transform\n",
    "            # Adjust path in CSV\n",
    "            self.df['path'] = self.df['path'].apply(lambda x: os.path.join(root, x))\n",
    "        def __len__(self): return len(self.df)\n",
    "        def __getitem__(self, idx):\n",
    "            path = self.df.iloc[idx]['path']\n",
    "            lbl = self.df.iloc[idx]['label']\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                if self.transform: img = self.transform(img)\n",
    "                return img, torch.tensor(lbl, dtype=torch.float32)\n",
    "            except:\n",
    "                return torch.zeros((3,224,224)), torch.tensor(-1.0)\n",
    "\n",
    "    ext_dataset = ExternalDataset(EXT_TEST_CSV, EXT_DATA_ROOT, val_transform)\n",
    "    ext_loader = DataLoader(ext_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "    all_preds_ext, all_labels_ext = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(ext_loader, desc=\"Ext Inference\"):\n",
    "            valid = (lbls != -1)\n",
    "            if not valid.any(): continue\n",
    "            imgs = imgs[valid].to(DEVICE)\n",
    "            with torch.amp.autocast(device_type='cuda', enabled=(DEVICE.type == \"cuda\")):\n",
    "                out = model(imgs)\n",
    "            probs = torch.sigmoid(out).squeeze().cpu().numpy().tolist()\n",
    "            if isinstance(probs, float): probs = [probs]\n",
    "            all_preds_ext.extend(probs)\n",
    "            all_labels_ext.extend(lbls[valid].numpy().tolist())\n",
    "            \n",
    "    all_labels_ext = np.array(all_labels_ext)\n",
    "    all_preds_probs_ext = np.array(all_preds_ext)\n",
    "    print(f\"‚úÖ External Inference Done: {len(all_labels_ext)} samples.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è External dataset not found. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc9268",
   "metadata": {},
   "source": [
    "# PHASE 4: ANALYSIS & VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e4407",
   "metadata": {},
   "source": [
    "#### Cell 10: Standard Performance Metrics\n",
    "#### ROC, Confusion Matrix, Classification Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b226dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 10: Standard Metrics ---\n",
    "print(\"--- Test Results (200k) ---\")\n",
    "acc = accuracy_score(all_labels_200k, all_preds_classes_200k)\n",
    "auc_score = roc_auc_score(all_labels_200k, all_preds_probs_200k)\n",
    "print(f\"Accuracy: {acc:.4f} | AUC: {auc_score:.4f}\")\n",
    "print(classification_report(all_labels_200k, all_preds_classes_200k, target_names=['Fake', 'Real']))\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(all_labels_200k, all_preds_probs_200k)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'AUC={auc_score:.4f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels_200k, all_preds_classes_200k)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc253559",
   "metadata": {},
   "source": [
    "#### Cell 11: Advanced Metric Visualizations\n",
    "#### PR Curve, Score Distributions, Pie Chart, Threshold Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e59f5",
   "metadata": {},
   "source": [
    "# --- Cell 11: Advanced Metrics ---\n",
    "\n",
    "# 1. PR Curve\n",
    "prec, rec, _ = precision_recall_curve(all_labels_200k, all_preds_probs_200k)\n",
    "ap = average_precision_score(all_labels_200k, all_preds_probs_200k)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# PR Curve\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(rec, prec, 'b-', label=f'AP={ap:.4f}')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.grid(True, ls=':')\n",
    "\n",
    "# Score Distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(all_preds_probs_200k[all_labels_200k==0], color='red', label='Fake', kde=True, stat='density')\n",
    "sns.histplot(all_preds_probs_200k[all_labels_200k==1], color='green', label='Real', kde=True, stat='density')\n",
    "plt.title('Score Distribution')\n",
    "plt.legend()\n",
    "\n",
    "# Pie Chart\n",
    "plt.subplot(1, 3, 3)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "plt.pie([tn, tp, fp, fn], labels=['TN (Fake)', 'TP (Real)', 'FP (Fake)', 'FN (Real)'], \n",
    "        colors=['#457B9D', '#A8DADC', '#E63946', '#F4A261'], explode=(0,0,0.1,0.1), autopct='%1.1f%%')\n",
    "plt.title('Outcome Breakdown')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Box Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "df_box = pd.DataFrame({'Score': all_preds_probs_200k, 'Label': all_labels_200k})\n",
    "df_box['Label'] = df_box['Label'].map({0:'Fake', 1:'Real'})\n",
    "sns.boxplot(x='Label', y='Score', data=df_box, palette={'Fake':'#E63946', 'Real':'#457B9D'})\n",
    "plt.title('Score Spread by Class')\n",
    "plt.grid(True, axis='y', ls=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b22e37",
   "metadata": {},
   "source": [
    "#### Cell 12: Qualitative Analysis (Errors & Uncertainty)\n",
    "#### Top 16 Errors and Most Uncertain Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2fb579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 12: Qualitative Analysis ---\n",
    "def get_img(idx):\n",
    "    t_img, lbl = test_dataset[idx]\n",
    "    img = t_img.permute(1,2,0).cpu().numpy()\n",
    "    img = np.clip((img * 0.5) + 0.5, 0, 1)\n",
    "    return img, lbl.item()\n",
    "\n",
    "errors = np.abs(all_labels_200k - all_preds_probs_200k)\n",
    "indices = np.arange(len(all_labels_200k))\n",
    "\n",
    "# 1. Worst Mistakes (High Confidence Errors)\n",
    "fp_mask = (all_labels_200k == 0)\n",
    "top_fp = indices[fp_mask][np.argsort(all_preds_probs_200k[fp_mask])[::-1]][:8] # Fake called Real\n",
    "\n",
    "fn_mask = (all_labels_200k == 1)\n",
    "top_fn = indices[fn_mask][np.argsort(all_preds_probs_200k[fn_mask])][:8] # Real called Fake\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle(\"Worst Errors: Top Row=False Positives, Bottom Row=False Negatives\")\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < 8: idx = top_fp[i]; type_err = \"FP (Fake->Real)\"\n",
    "    else:     idx = top_fn[i-8]; type_err = \"FN (Real->Fake)\"\n",
    "    \n",
    "    img, lbl = get_img(idx)\n",
    "    score = all_preds_probs_200k[idx]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{type_err}\\nScore: {score:.4f}\", color='red')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 2. Uncertainty (Scores near 0.5)\n",
    "uncertain_mask = (all_preds_probs_200k > 0.45) & (all_preds_probs_200k < 0.55)\n",
    "uncertain_idx = indices[uncertain_mask][:8]\n",
    "\n",
    "if len(uncertain_idx) > 0:\n",
    "    fig, axes = plt.subplots(1, len(uncertain_idx), figsize=(16, 3))\n",
    "    fig.suptitle(\"Most Uncertain Images (Score ~ 0.5)\")\n",
    "    for i, idx in enumerate(uncertain_idx):\n",
    "        img, lbl = get_img(idx)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"True: {lbl}\\nScore: {all_preds_probs_200k[idx]:.4f}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f3a7f",
   "metadata": {},
   "source": [
    "#### Cell 13: Explainability (Saliency & FFT)\n",
    "#### Uses Captum for heatmaps and standard FFT analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2cc05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 13: Explainability ---\n",
    "from captum.attr import IntegratedGradients, visualization as viz\n",
    "\n",
    "# 1. Saliency Maps\n",
    "# Wrapper to handle shape mismatch in Captum\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, m): super().__init__(); self.m = m\n",
    "    def forward(self, x): \n",
    "        out = self.m(x)\n",
    "        return out.unsqueeze(-1) if out.dim()==1 else out\n",
    "\n",
    "print(\"Generating Saliency Maps...\")\n",
    "model.eval()\n",
    "wrapped = ModelWrapper(model).to(DEVICE)\n",
    "ig = IntegratedGradients(wrapped)\n",
    "\n",
    "viz_indices = random.sample(range(len(test_dataset)), 4)\n",
    "fig, axes = plt.subplots(4, 2, figsize=(8, 16))\n",
    "\n",
    "for i, idx in enumerate(viz_indices):\n",
    "    t_img, lbl = test_dataset[idx]\n",
    "    inp = t_img.unsqueeze(0).to(DEVICE)\n",
    "    attr = ig.attribute(inp, target=0, n_steps=50)\n",
    "    \n",
    "    orig = t_img.permute(1,2,0).numpy()\n",
    "    orig = np.clip((orig * 0.5) + 0.5, 0, 1)\n",
    "    hm = attr.cpu().squeeze().permute(1,2,0).detach().numpy()\n",
    "    \n",
    "    axes[i][0].imshow(orig)\n",
    "    axes[i][0].set_title(f\"Label: {lbl.item()}\")\n",
    "    axes[i][0].axis('off')\n",
    "    \n",
    "    viz.visualize_image_attr(hm, orig, method=\"blended_heat_map\", sign=\"all\", \n",
    "                             show_colorbar=True, plt_fig_axis=(fig, axes[i][1]), use_pyplot=False)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 2. FFT Analysis\n",
    "print(\"Generating FFT Spectrum...\")\n",
    "def get_avg_fft(label_cls, n=300):\n",
    "    accum = None\n",
    "    count = 0\n",
    "    for i in range(len(test_dataset)):\n",
    "        if count >= n: break\n",
    "        t_img, lbl = test_dataset[i]\n",
    "        if lbl.item() != label_cls: continue\n",
    "        \n",
    "        img = (t_img.cpu().numpy() * 0.5) + 0.5\n",
    "        img = (np.clip(img,0,1).transpose(1,2,0) * 255).astype(np.uint8)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        f = np.fft.fft2(gray)\n",
    "        fshift = np.fft.fftshift(f)\n",
    "        mag = 20 * np.log(np.abs(fshift) + 1)\n",
    "        \n",
    "        accum = mag if accum is None else accum + mag\n",
    "        count += 1\n",
    "    return accum / count\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1); plt.imshow(get_avg_fft(0), cmap='magma'); plt.title('Avg FFT (Fake)')\n",
    "plt.subplot(1,2,2); plt.imshow(get_avg_fft(1), cmap='magma'); plt.title('Avg FFT (Real)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a69c99c",
   "metadata": {},
   "source": [
    "#### Cell 14: Robustness & Benchmarking\n",
    "#### Corruptions check and Ablation comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 14.0: Baseline Training & Inference ---\n",
    "\n",
    "print(\"‚è≥ STARTING BASELINE TRAINING (ResNet-Only) for Comparison...\")\n",
    "\n",
    "# 1. Define Baseline Architecture (Exact same, just without Mamba)\n",
    "class ResNetBaseline(nn.Module):\n",
    "    def __init__(self, out_dim=128):\n",
    "        super().__init__()\n",
    "        # Use exact same backbone setup\n",
    "        self.net = models.resnet18(weights=None) \n",
    "        self.net.fc = nn.Identity()\n",
    "        self.proj = nn.Linear(512, out_dim)\n",
    "        \n",
    "        # Init backbone weights same as main model\n",
    "        nn.init.kaiming_normal_(self.proj.weight, nonlinearity='relu')\n",
    "        if self.proj.bias is not None: nn.init.zeros_(self.proj.bias)\n",
    "\n",
    "        # Same Classifier Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(out_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        # Init classifier weights same as main model\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.proj(self.net(x)) # (B, 128)\n",
    "        # Skip Mamba Block entirely\n",
    "        return self.classifier(feat).squeeze(-1)\n",
    "\n",
    "# 2. Setup Baseline Training\n",
    "model_baseline = ResNetBaseline(out_dim=128).to(DEVICE)\n",
    "opt_baseline = optim.AdamW(model_baseline.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "crit_baseline = nn.BCEWithLogitsLoss()\n",
    "scaler_bl = torch.amp.GradScaler(enabled=(DEVICE.type == \"cuda\"))\n",
    "\n",
    "# 3. Training Loop (Exact duplicate of main loop for fairness)\n",
    "# We train for the same EPOCHS to ensure fair comparison\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model_baseline.train()\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Baseline Epoch {epoch}\", leave=False):\n",
    "        xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
    "        opt_baseline.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda', enabled=(DEVICE.type == \"cuda\")):\n",
    "            out = model_baseline(xb)\n",
    "            loss = crit_baseline(out, yb)\n",
    "        \n",
    "        scaler_bl.scale(loss).backward()\n",
    "        scaler_bl.step(opt_baseline)\n",
    "        scaler_bl.update()\n",
    "\n",
    "print(\"‚úÖ Baseline Training Complete.\")\n",
    "\n",
    "# 4. Run Inference for Baseline\n",
    "print(\"üß™ Evaluating Baseline on Test Set...\")\n",
    "bl_preds, bl_trues = [], []\n",
    "model_baseline.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in tqdm(test_loader, desc=\"Baseline Inference\"):\n",
    "        valid = (yb != -1)\n",
    "        if not valid.any(): continue\n",
    "        xb = xb[valid].to(DEVICE)\n",
    "        \n",
    "        with torch.amp.autocast(device_type='cuda', enabled=(DEVICE.type == \"cuda\")):\n",
    "            out = model_baseline(xb)\n",
    "        \n",
    "        probs = torch.sigmoid(out).squeeze().cpu().numpy().tolist()\n",
    "        if isinstance(probs, float): probs = [probs]\n",
    "        \n",
    "        bl_preds.extend(probs)\n",
    "        bl_trues.extend(yb[valid].cpu().numpy().tolist())\n",
    "\n",
    "# 5. Calculate Metrics\n",
    "bl_trues = np.array(bl_trues)\n",
    "bl_probs = np.array(bl_preds)\n",
    "bl_classes = (bl_probs > 0.5).astype(int)\n",
    "\n",
    "bl_auc = roc_auc_score(bl_trues, bl_probs)\n",
    "bl_ap  = average_precision_score(bl_trues, bl_probs)\n",
    "bl_f1  = f1_score(bl_trues, bl_classes)\n",
    "bl_acc = accuracy_score(bl_trues, bl_classes)\n",
    "\n",
    "# 6. Store in the variable needed for Cell 14.1\n",
    "baseline_metrics = [bl_auc, bl_ap, bl_f1, bl_acc]\n",
    "\n",
    "print(f\"\\n--- Baseline Results (ResNet-Only) ---\")\n",
    "print(f\"AUC: {bl_auc:.4f} | AP: {bl_ap:.4f} | F1: {bl_f1:.4f} | Acc: {bl_acc:.4f}\")\n",
    "print(\"Values saved to 'baseline_metrics' for Plot 14.\")\n",
    "\n",
    "# Save the Baseline Model just in case\n",
    "torch.save(model_baseline.state_dict(), os.path.join(CKPT_DIR, \"baseline_resnet_final.pth\"))\n",
    "print(\"üíæ Baseline model saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8bc8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 14.1: Robustness Analysis & Ablation Study ---\n",
    "\n",
    "# 1. Robustness (Blur/Noise)\n",
    "levels = [0, 1, 3, 5]\n",
    "blur_aucs = []\n",
    "\n",
    "print(\"Running Robustness Check (Blur)...\")\n",
    "for k in levels:\n",
    "    if k == 0: \n",
    "        blur_aucs.append(auc_score)\n",
    "        continue\n",
    "        \n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(test_loader, desc=f\"Blur k={k}\", leave=False):\n",
    "            # Apply blur on tensor directly\n",
    "            imgs = transforms.functional.gaussian_blur(imgs, kernel_size=(k*2+1, k*2+1))\n",
    "            imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
    "            out = model(imgs)\n",
    "            preds.extend(torch.sigmoid(out).cpu().numpy().tolist())\n",
    "            trues.extend(lbls.cpu().numpy().tolist())\n",
    "    blur_aucs.append(roc_auc_score(trues, preds))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(levels, blur_aucs, 'o-', color='purple')\n",
    "plt.title('Robustness to Blur')\n",
    "plt.xlabel('Blur Intensity')\n",
    "plt.ylabel('AUC')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2. Ablation Study\n",
    "our_metrics = [auc_score, ap, f1_score(all_labels_200k, all_preds_classes_200k), acc]\n",
    "labels = ['AUC', 'AP', 'F1', 'Acc']\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x - width/2, our_metrics, width, label='ResNet+Mamba (Ours)', color='navy')\n",
    "plt.bar(x + width/2, baseline_metrics, width, label='ResNet Baseline', color='skyblue')\n",
    "plt.xticks(x, labels)\n",
    "plt.title('Ablation Study: Ours vs Baseline')\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', ls=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5881e08",
   "metadata": {},
   "source": [
    "#### Cell 15: Efficiency Analysis\n",
    "#### Calculates Latency and FLOPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc9f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 15: Efficiency Analysis ---\n",
    "def measure_efficiency(model, device):\n",
    "    dummy = torch.randn(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    # 1. Latency\n",
    "    model.eval()\n",
    "    for _ in range(50): _ = model(dummy) # Warmup\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(100): _ = model(dummy)\n",
    "    torch.cuda.synchronize()\n",
    "    latency = ((time.time() - start) / 100) * 1000 # ms\n",
    "    \n",
    "    # 2. FLOPs (on CPU to allow counting)\n",
    "    try:\n",
    "        model_cpu = model.__class__(embed_dim=128, d_state=64, device='cpu')\n",
    "        flops, params = profile(model_cpu, inputs=(torch.randn(1, 3, 224, 224),), verbose=False)\n",
    "    except:\n",
    "        flops, params = 0, 0\n",
    "        \n",
    "    return latency, flops/1e9, params/1e6\n",
    "\n",
    "lat, gflops, params = measure_efficiency(model, DEVICE)\n",
    "\n",
    "print(f\"--- Efficiency Stats ---\")\n",
    "print(f\"Latency: {lat:.2f} ms\")\n",
    "print(f\"GFLOPs:  {gflops:.2f}\")\n",
    "print(f\"Params:  {params:.2f} M\")\n",
    "\n",
    "df_eff = pd.DataFrame([\n",
    "    {'Metric': 'Latency (ms)', 'Value': lat},\n",
    "    {'Metric': 'GFLOPs', 'Value': gflops},\n",
    "    {'Metric': 'Params (M)', 'Value': params}\n",
    "])\n",
    "sns.barplot(x='Metric', y='Value', data=df_eff)\n",
    "plt.title('Model Efficiency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
